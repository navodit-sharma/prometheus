groups:
- name: k8s-container.rules
  rules:

  - alert: KubernetesContainerOomKilled
    expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
    for: 0m
    labels:
      severity: warning
    annotations:
      summary: Kubernetes container oom killed
      description: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes. Check dashboard - {{ .ExternalURL | reReplaceAll "(.*)/prometheus" "${1}" }}/d/kubernetes-pods/kubernetes-pods?var-datasource=Prometheus&var-cluster={{ $labels.cluster }}&var-namespace={{ $labels.namespace }}&var-pod={{ $labels.pod }}&var-container={{ $labels.container }}

  # - alert: ContainerKilled
  #   expr: time() - container_last_seen > 60
  #   for: 0m
  #   labels:
  #     severity: warning
  #   annotations:
  #     summary: Container killed
  #     description: "A container has disappeared."
  #
  #
  # cAdvisor can sometimes consume a lot of CPU, so this alert will fire constantly.
  # If you want to exclude it from this alert, exclude the series having an empty name: container_cpu_usage_seconds_total{name!=""}

  - alert: ContainerCpuUsageHigh
    expr: (sum(rate(container_cpu_usage_seconds_total{cluster=~"",namespace=~"shared",container!~"",container!~"POD",image!~""}[3m])) by (container, namespace, pod) / sum(kube_pod_container_resource_limits{cluster=~"",namespace=~"shared",unit="core"}) by (container, namespace,pod)) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: Container CPU usage high
      description: Container CPU usage is above 80% for last 5m. Check dashboard - {{ .ExternalURL | reReplaceAll "(.*)/prometheus" "${1}" }}/d/kubernetes-pods/kubernetes-pods?var-datasource=Prometheus&var-cluster={{ $labels.cluster }}&var-namespace={{ $labels.namespace }}&var-pod={{ $labels.pod }}&var-container={{ $labels.container }}

  # See https://medium.com/faun/how-much-is-too-much-the-linux-oomkiller-and-used-memory-d32186f29c9d
  - alert: ContainerMemoryUsageHigh
    expr: (sum(container_memory_working_set_bytes{container!=""}) BY (instance,container,image,name,namespace,node,pod) / sum(container_spec_memory_limit_bytes{container!=""} > 0) BY (instance,container,image,name,namespace,node,pod) * 100) > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: Container Memory usage high
      description: Container Memory usage is above 80% for last 5m. Check dashboard - {{ .ExternalURL | reReplaceAll "(.*)/prometheus" "${1}" }}/d/kubernetes-pods/kubernetes-pods?var-datasource=Prometheus&var-cluster={{ $labels.cluster }}&var-namespace={{ $labels.namespace }}&var-pod={{ $labels.pod }}&var-container={{ $labels.container }}

  # - alert: ContainerVolumeUsage
  #   expr: (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance))) * 100 > 80
  #   for: 2m
  #   labels:
  #     severity: warning
  #   annotations:
  #     summary: Container Volume usage
  #     description: "Container Volume usage is above 80%."
  # - alert: ContainerVolumeIoUsage
  #   expr: (sum(container_fs_io_current) BY (instance, name) * 100) > 80
  #   for: 2m
  #   labels:
  #     severity: warning
  #   annotations:
  #     summary: Container Volume IO usage
  #     description: "Container Volume IO usage is above 80%."

  - alert: ContainerHighThrottleRate
    expr: rate(container_cpu_cfs_throttled_seconds_total{name!=""}[3m]) > 1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: Container high throttle rate
      description: Container is being throttled for last 3m. Check dashboard - {{ .ExternalURL | reReplaceAll "(.*)/prometheus" "${1}" }}/d/kubernetes-pods/kubernetes-pods?var-datasource=Prometheus&var-cluster={{ $labels.cluster }}&var-namespace={{ $labels.namespace }}&var-pod={{ $labels.pod }}&var-container={{ $labels.container }}
