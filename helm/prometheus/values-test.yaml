---

global:
  grafanaAdminPassword: "notInUse-random-value-to-pass-schema-validation"
  smtpApiKey: "notInUse-random-value-to-pass-schema-validation"
  slackApiUrl: "notInUse-random-value-to-pass-schema-validation"
  externalDomain: "ersolab-monitor.valamis.io"
  smtpDestinationAddress: "navodit.sharma@valamis.com"
  web:
    ssl:
      enabled: true
      externalSecret:
        enabled: true
        esoMigrated: true
        keyVaultName: "v4-ersolab-kv"

prometheus-elasticsearch-exporter:
  resources:
    requests:
      cpu: 2000m
      memory: 2048Mi
    limits:
      cpu: 2000m
      memory: 2048Mi

customResources:
  prometheusCustomAlertRules:
    create: true
  grafanaCustomDashboard:
    create: true

emissary-ingress:
  service:
    loadBalancerIP: "20.61.88.14"

kube-prometheus-stack:
  alertmanager:
    customConfig: |
      global:
        resolve_timeout: 5m
        {{- if and (.Values.global.web.ssl.externalSecret.enabled) (.Values.global.web.ssl.externalSecret.esoMigrated) }}
        slack_api_url: '{{`{{`}} .slackapiurl {{`}}`}}'
        {{- else }}
        slack_api_url: '{{ .Values.global.slackApiUrl }}'
        {{- end }}
      inhibit_rules:
      - equal:
        - namespace
        - alertname
        source_matchers:
        - severity = critical
        target_matchers:
        - severity =~ warning|info
      - equal:
        - namespace
        - alertname
        source_matchers:
        - severity = warning
        target_matchers:
        - severity = info
      - equal:
        - namespace
        source_matchers:
        - alertname = InfoInhibitor
        target_matchers:
        - severity = info
      receivers:
      - name: "null"
      - name: email-support
        email_configs:
        - auth_identity: apikey
          auth_username: apikey
          from: alerts@valamis.com
          {{- if and (.Values.global.web.ssl.externalSecret.enabled) (.Values.global.web.ssl.externalSecret.esoMigrated) }}
          auth_password: {{`{{`}} .smtpapikey {{`}}`}}
          headers:
            From: Valamis Alert Manager<alerts@valamis.com>
            Subject: '{{`{{`}}`{{`{{`}}`}} .CommonLabels.alertname {{`{{`}}`}}`{{`}}`}} - [ Cluster = {{`{{`}}`{{`{{`}}`}} .CommonLabels.cluster {{`{{`}}`}}`{{`}}`}} ]'
            To: "{{ .Values.global.smtpDestinationAddress }}"
          html: '{{`{{`}}`{{`{{`}}`}} template "email.default.html" . {{`{{`}}`}}`{{`}}`}}'
          {{- else }}
          auth_password: {{ .Values.global.smtpApiKey }}
          headers:
            From: Valamis Alert Manager<alerts@valamis.com>
            Subject: '{{`{{`}} .CommonLabels.alertname {{`}}`}} - [ Cluster = {{`{{`}} .CommonLabels.cluster {{`}}`}} ]'
            To: "{{ .Values.global.smtpDestinationAddress }}"
          html: ''{{`{{`}} template "email.default.html" . {{`}}`}}''
          {{- end }}
          require_tls: true
          send_resolved: false
          smarthost: smtp.sendgrid.net:587
          to: "{{ .Values.global.smtpDestinationAddress }}"
      - name: slack
        slack_configs:
        - channel: '#prometheus-alerts'
          send_resolved: false
          {{- if and (.Values.global.web.ssl.externalSecret.enabled) (.Values.global.web.ssl.externalSecret.esoMigrated) }}
          title: '{{`{{`}}`{{`{{`}}`}} .CommonLabels.alertname {{`{{`}}`}}`{{`}}`}} - [ Cluster = {{`{{`}}`{{`{{`}}`}} .CommonLabels.cluster {{`{{`}}`}}`{{`}}`}} ]'
          text: |-
            {{`{{`}}`{{`{{`}}`}} range .Alerts -{{`{{`}}`}}`{{`}}`}}
            *Alert:* {{`{{`}}`{{`{{`}}`}} .Annotations.summary {{`{{`}}`}}`{{`}}`}}{{`{{`}}`{{`{{`}}`}} if .Labels.severity {{`{{`}}`}}`{{`}}`}} - `{{`{{`}}`{{`{{`}}`}} .Labels.severity {{`{{`}}`}}`{{`}}`}}`{{`{{`}}`{{`{{`}}`}} end {{`{{`}}`}}`{{`}}`}}
            *Description:* {{`{{`}}`{{`{{`}}`}} .Annotations.description {{`{{`}}`}}`{{`}}`}}
            *Dashboard:* {{`{{`}}`{{`{{`}}`}} .Annotations.dashboard {{`{{`}}`}}`{{`}}`}}
            *Details:*
              {{`{{`}}`{{`{{`}}`}} range .Labels.SortedPairs {{`{{`}}`}}`{{`}}`}} • *{{`{{`}}`{{`{{`}}`}} .Name {{`{{`}}`}}`{{`}}`}}:* `{{`{{`}}`{{`{{`}}`}} .Value {{`{{`}}`}}`{{`}}`}}`
              {{`{{`}}`{{`{{`}}`}} end {{`{{`}}`}}`{{`}}`}}
            {{`{{`}}`{{`{{`}}`}} end {{`{{`}}`}}`{{`}}`}}
          {{- else }}
          title: '{{`{{`}} .CommonLabels.alertname {{`}}`}} - [ Cluster = {{`{{`}} .CommonLabels.cluster {{`}}`}} ]'
          text: |-
            {{`{{`}} range .Alerts -{{`}}`}}
            *Alert:* {{`{{`}} .Annotations.summary {{`}}`}}{{`{{`}} if .Labels.severity {{`}}`}} - `{{`{{`}} .Labels.severity {{`}}`}}`{{`{{`}} end {{`}}`}}
            *Description:* {{`{{`}} .Annotations.description {{`}}`}}
            *Dashboard:* {{`{{`}} .Annotations.dashboard {{`}}`}}
            *Details:*
              {{`{{`}} range .Labels.SortedPairs {{`}}`}} • *{{`{{`}} .Name {{`}}`}}:* `{{`{{`}} .Value {{`}}`}}`
              {{`{{`}} end {{`}}`}}
            {{`{{`}} end {{`}}`}}
            {{- end }}
      route:
        receiver: "null"
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 5m
        group_by: [alertname]
        routes:
        - receiver: email-support
          matchers:
          - alertname=~"KubernetesClientCertificateExpiresNextWeek|KubernetesClientCertificateExpiresSoon|KubernetesNodeNotReady|PersistentVolumeUtilizationHigh|PersistentVolumeUtilizationCritical|PersistentVolumeFull|PersistentvolumeclaimPending|PersistentvolumeError|KubernetesContainerOomKilled|KubernetesDaemonsetRolloutStuck|KubernetesDeploymentReplicasMismatch|KafkaTopicsReplicasNotInSync|KafkaConsumerLag|KafkaConsumerLagAnalytics|KubernetesPodNotReady|KubernetesPodPending|KubernetesPodFailed|KubernetesPodStateUnknown|KubernetesPodCrashLoop"
        - receiver: slack
          matchers:
          - alertname=~"KubernetesClientCertificateExpiresNextWeek|KubernetesClientCertificateExpiresSoon|KubernetesNodeNotReady|PersistentVolumeUtilizationHigh|PersistentVolumeUtilizationCritical|PersistentVolumeFull|PersistentvolumeclaimPending|PersistentvolumeError|KubernetesContainerOomKilled|KubernetesDaemonsetRolloutStuck|KubernetesDeploymentReplicasMismatch|KubernetesReplicassetMismatch|KafkaTopicsReplicasNotInSync|KafkaConsumerLag|KafkaConsumerLagAnalytics|KubernetesPodNotReady|KubernetesPodPending|KubernetesPodFailed|KubernetesPodStateUnknown|KubernetesPodCrashLoop"
      templates:
      - /etc/alertmanager/config/*.tmpl
    alertmanagerSpec:
      externalUrl: "http://ersolab-monitor.valamis.io/alertmanager"
  grafana:
    resources:
      limits:
        cpu: 1500m
        memory: 2048Mi
      requests:
        cpu: 500m
        memory: 2048Mi
    persistence:
      size: 15Gi
    env:
      GF_SMTP_ENABLED: "true"
  prometheus:
    prometheusSpec:
      resources:
        limits:
          cpu: 2500m
          memory: 6144Mi
        requests:
          cpu: 1250m
          memory: 6144Mi
      externalLabels:
        cluster: "ValamisErsolab"
      storageSpec:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 150Gi
